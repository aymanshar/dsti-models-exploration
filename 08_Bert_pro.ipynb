{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "08-Bert_pro.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1hLUGMgySks0Smk-w--ss0jI3NoZHtNf1",
      "authorship_tag": "ABX9TyNwUiIjyiu448lI2BJIomeX",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sereniiti/models-exploration/blob/develop/Ayman/08_Bert_pro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FktPcryGrr6l"
      },
      "source": [
        "**Seneriiti with Bert** \n",
        "\n",
        "### Dataset:\n",
        "The dataset we will contains sentences with 7 categories (from very violent to excellent).\n",
        "\n",
        "**Summary:**\n",
        "1. !pip install transformers\n",
        "2. import data set\n",
        ">- check and clean data set from Null values converting to 0 or removing the records.\n",
        ">- check and remove doyplicated sentences from dataset.\n",
        ">- map category column records from text to numbers.\n",
        "\n",
        "3. Loading the Pre-trained BERT model DistilBERT or BERT importing import transformers\n",
        "4. Load pretrained model/tokenizer\n",
        "5. Preparing the Dataset before using with bert (Tokenization, Padding defining the number of records we are going to train and test, Masking)\n",
        "6. create an input tensor out of the padded token matrix, and send that to DistilBERT\n",
        "7. split our datset into a training set and testing set (from sklearn.model_selection import train_test_split)\n",
        "8. train the LogisticRegression model\n",
        "9. use confusion_matrix to figure out how is our training accuracy, we can use also (pd.crosstab(test_labels, predict_md, rownames=['True'], colnames=['Predicted'], margins=True))\n",
        "10. predict test input text\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0tBcj1Uv4kj"
      },
      "source": [
        "# Install transformers library which provides us with an implementation of DistilBERT as well as pretrained versions of the model.\n",
        "!pip install transformers -q gwpy &> /dev/null"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lq2rmlFVS7lj",
        "outputId": "5ad43249-ce90-4232-cebe-1b08e50b06a8"
      },
      "source": [
        "# to mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Hide warnings if there are any\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/sereniiti_project')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jFD4_FYuo4u"
      },
      "source": [
        "## Importing and prepare dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wdhphjEwj03Y"
      },
      "source": [
        "# import pandas\n",
        "import pandas as pd\n",
        "\n",
        "# import the csv file in df data frame\n",
        "df = pd.read_csv('/content/drive/MyDrive/sereniiti_project/sereniiti_db.csv',sep=';',index_col='main_id_pk')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAK5r2fJWwIY"
      },
      "source": [
        "import smodel"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y0pmceyGDZtj"
      },
      "source": [
        "#preprocessing dataset sentences.\n",
        "list=[]\n",
        "for item in df['text_en']:\n",
        "  list.append(smodel.text_preprocessing(item))\n",
        "df['words']=list"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCoYK6tGwFyB"
      },
      "source": [
        "#encode category variable into numeric\n",
        "df.step_id_fk= pd.Categorical(df.step_id_fk)\n",
        "df['step_code'] = df.step_id_fk.cat.codes\n",
        "df=df[['words','rating_id_fk','step_id_fk','step_code']]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QLh7sa1UewNH",
        "outputId": "123e0a73-b138-4b87-e2e1-0f5f53bd1bed"
      },
      "source": [
        "smodel.bert_model_mcat('light', pd, df['words'][:3000], df['rating_id_fk'][:3000],df['step_code'][:3000],'train',None )"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Transformation: creating model instance ... steps\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_transform.weight']\n",
            "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "-------------------Transformation Done -------------------\n",
            "Transform column to tokens words then numbers \n",
            "----------------------------------\n",
            "\n",
            "by tokenized firs 3 records like this : \n",
            "\n",
            " main_id_pk\n",
            "1    [101, 3438, 2199, 18047, 2015, 1998, 2017, 218...\n",
            "2    [101, 1037, 2297, 2817, 8920, 1996, 3896, 1997...\n",
            "3    [101, 1037, 2502, 3124, 2066, 2017, 3791, 2000...\n",
            "Name: words, dtype: object\n",
            "\n",
            "===============================================================\n",
            "\n",
            "Berting padding and attention mask ... step \n",
            "----------------------------------\n",
            "\n",
            " padding ... step \n",
            "----------------------\n",
            "\n",
            "padded shape:  (3000, 108)\n",
            "\n",
            " masking ... step \n",
            "----------------------\n",
            "\n",
            "attention_mask shape:  (3000, 108)\n",
            "\n",
            "tensoring ... step \n",
            "----------------------\n",
            "\n",
            "input_ids \n",
            " ----------------------\n",
            " tensor([[ 101, 3438, 2199,  ...,    0,    0,    0],\n",
            "        [ 101, 1037, 2297,  ...,    0,    0,    0],\n",
            "        [ 101, 1037, 2502,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [ 101, 1045, 2031,  ...,    0,    0,    0],\n",
            "        [ 101, 1045, 2031,  ...,    0,    0,    0],\n",
            "        [ 101, 1045, 2031,  ...,    0,    0,    0]])\n",
            "\n",
            "attention_mask \n",
            " ----------------------\n",
            " tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0]])\n",
            "\n",
            "last_hidden \n",
            " ----------------------\n",
            " tensor([[-0.1367, -0.2271,  0.1057,  ..., -0.2264,  0.3254,  0.3519],\n",
            "        [-0.2766, -0.1106, -0.3935,  ..., -0.1644,  0.1665,  0.3392],\n",
            "        [-0.1275,  0.0198, -0.0396,  ..., -0.1160,  0.2153,  0.2404],\n",
            "        ...,\n",
            "        [-0.0448,  0.0023,  0.0134,  ..., -0.0757,  0.2873,  0.1863],\n",
            "        [ 0.0377,  0.1364, -0.1475,  ..., -0.0154,  0.2920,  0.2674],\n",
            "        [-0.0243, -0.0569,  0.0176,  ..., -0.0586,  0.1930,  0.1425]])\n",
            "\n",
            "preparing features and labels ... step \n",
            "-------------------------\n",
            "\n",
            "len of train_features:  2250 \n",
            "len of test_features:  750\n",
            "\n",
            "len of train_features:  2250 \n",
            "len of test_features:  750\n",
            "\n",
            "logisticRegression for the training set ... step \n",
            "==============================\n",
            "\n",
            "predict the test features ... step \n",
            "============================\n",
            "\n",
            "accuracy score1:  0.7013333333333334 \n",
            "accuracy score2:  0.7533333333333333\n",
            "\n",
            " Confusion_matrix : \n",
            " --------------------\n",
            "\n",
            " Predicted   0   1   2  3    4    5  All\n",
            "True                                   \n",
            "0          43  21   6  0    7    0   77\n",
            "1          19  44  16  0   15    5   99\n",
            "2           6  14  40  4   13   11   88\n",
            "3           0   0   2  3    1   10   16\n",
            "4           7   4   7  1  277   26  322\n",
            "5           2   1   2  1   23  119  148\n",
            "All        77  84  73  9  336  171  750\n",
            "\n",
            " Confusion_matrix2 : \n",
            " --------------------\n",
            "\n",
            " Predicted   1    4   5  6  7  9  10  14  19  All\n",
            "True                                            \n",
            "1          42    7   8  0  0  0   0   0   0   57\n",
            "2           0    1   0  0  0  0   0   0   0    1\n",
            "4          10  407  15  1  0  1   1   9   0  444\n",
            "5          19   31  56  2  1  1   0   0   0  110\n",
            "6           1    2   3  3  0  0   0   0   0    9\n",
            "8           0    2   1  0  0  0   0   1   0    4\n",
            "9           0    2   3  0  0  2   0   0   0    7\n",
            "10          0    7   0  0  0  0   1   1   1   10\n",
            "11          0    3   0  0  0  0   0   0   0    3\n",
            "14          0   23   0  1  0  0   1  36   5   66\n",
            "15          0    0   0  0  0  0   0   1   0    1\n",
            "19          0   11   0  0  0  0   0   4  18   33\n",
            "20          0    0   0  0  0  1   0   0   1    2\n",
            "21          0    1   0  0  0  0   0   0   0    1\n",
            "22          0    0   0  0  0  0   0   2   0    2\n",
            "All        72  497  86  7  1  5   3  54  25  750\n",
            "\n",
            "====================================\n",
            "\n",
            "\n",
            "------Training Donne-----\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGrnRELom7w3"
      },
      "source": [
        "def predict_con_multiline(txt):\n",
        "  x_new_df=pd.DataFrame({'token':txt.split(\",\")})\n",
        "  \n",
        "  print ('\\nyour inquiry for : \\n', x_new_df['token'],'\\n\\n...... please wait..., it will take some time\\n' )\n",
        "  smodel.bert_model_mcat('light', pd, df['words'][:3000], df['rating_id_fk'][:3000],df['step_code'][:3000],'test',x_new_df['token'] )\n",
        "  x_new_df['predicting']= smodel.predict_md_test\n",
        "  x_new_df['ratings']=x_new_df['predicting'].map({0:'very likely to elicit a defensive reaction or high pain in other people', 1:'likely to elicit a defensive reaction in other people',2:'slight tendance to elicit defensiveness in other people', 3:'slight tendance to encourage dialogue',4:'likely to encourage an open dialogue or neutral',5:'very likely to encourage an open dialogue which is mutually satisfying'})\n",
        "\n",
        "  x_new_df['predicting_step']= smodel.predict_md_test2\n",
        "  x_new_df['classification']=x_new_df['predicting_step'].map({0:'Facts, needs',1:'emotions',2:'emotions, needs',3:'emotions, propositions',4:'facts',5:'facts, emotions',6:'facts, emotions, needs',7:'facts, emotions, needs, propositions',8:'facts, emotions, propositions',9:'facts, feelings  ',10:'facts, intentions',11:'facts, needs',12:'facts, needs, feelings', 13:'facts, needs, propositions',14:'facts, propositions',15:'facts, propositions, intentions',16:'feelings',17:'intentions',18:'needs',19:'propositions',20:'propositions, feelings',21:'propositions, intentions',22:'propositions, needs',23:'requests' })  \n",
        "  x_new_df['result']='\\n'+x_new_df['token']+' \\n**rating as: '+ x_new_df['ratings'] +' \\n**Classified as: '+ x_new_df['classification'] +'\\n'\n",
        "\n",
        "  print ('\\nResult abelow: \\n', '====================\\n')\n",
        "  for item in x_new_df['result']:\n",
        "    print(item)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xdvus3YOhpP_",
        "outputId": "b453236a-ace1-4639-fc6d-8b607042cf39"
      },
      "source": [
        "# data entry test model\n",
        "#========================\n",
        "#input the data for testing the model\n",
        "X_new = input('please input your text to test : ')\n",
        "\n",
        "predict_con_multiline(X_new)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "please input your text to test : You inspire me., You listen to me and you shut up, You make my heart warm., You make me ashamed, go hide.\n",
            "\n",
            "your inquiry for : \n",
            " 0                      You inspire me.\n",
            "1     You listen to me and you shut up\n",
            "2              You make my heart warm.\n",
            "3                  You make me ashamed\n",
            "4                             go hide.\n",
            "Name: token, dtype: object \n",
            "\n",
            "...... please wait..., it will take some time\n",
            "\n",
            "Transform column to tokens words then numbers \n",
            "----------------------------------\n",
            "\n",
            "by tokenized firs 3 records like this : \n",
            "\n",
            " 0                  [101, 2017, 18708, 2033, 1012, 102]\n",
            "1    [101, 2017, 4952, 2000, 2033, 1998, 2017, 3844...\n",
            "2       [101, 2017, 2191, 2026, 2540, 4010, 1012, 102]\n",
            "Name: token, dtype: object\n",
            "\n",
            "===============================================================\n",
            "\n",
            " padding ... step \n",
            "----------------------\n",
            "\n",
            "padded shape:  (5, 108)\n",
            "\n",
            " masking ... step \n",
            "----------------------\n",
            "\n",
            "attention_mask shape:  (5, 108)\n",
            "\n",
            "input_ids \n",
            " ----------------------\n",
            " tensor([[  101,  2017, 18708,  2033,  1012,   102,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0],\n",
            "        [  101,  2017,  4952,  2000,  2033,  1998,  2017,  3844,  2039,   102,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0],\n",
            "        [  101,  2017,  2191,  2026,  2540,  4010,  1012,   102,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0],\n",
            "        [  101,  2017,  2191,  2033, 14984,   102,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0],\n",
            "        [  101,  2175,  5342,  1012,   102,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0]])\n",
            "\n",
            "attention_mask \n",
            " ----------------------\n",
            " tensor([[1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
            "\n",
            "preparing features and labels ... step \n",
            "-------------------------\n",
            "\n",
            "predict the test features ... step \n",
            "============================\n",
            "\n",
            "Result abelow: \n",
            " ====================\n",
            "\n",
            "\n",
            "You inspire me. \n",
            "**rating as: likely to encourage an open dialogue or neutral \n",
            "**Classified as: facts\n",
            "\n",
            "\n",
            " You listen to me and you shut up \n",
            "**rating as: likely to elicit a defensive reaction in other people \n",
            "**Classified as: facts, propositions\n",
            "\n",
            "\n",
            " You make my heart warm. \n",
            "**rating as: likely to encourage an open dialogue or neutral \n",
            "**Classified as: facts, emotions\n",
            "\n",
            "\n",
            " You make me ashamed \n",
            "**rating as: slight tendance to elicit defensiveness in other people \n",
            "**Classified as: facts\n",
            "\n",
            "\n",
            " go hide. \n",
            "**rating as: likely to elicit a defensive reaction in other people \n",
            "**Classified as: propositions\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whXAtfIhIbcd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}